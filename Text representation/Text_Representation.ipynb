{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q8687QEMROb8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### One-Hot Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8687QEMROb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue6_ITFPwpdl",
        "outputId": "f56dddcb-99e5-4727-91f6-47f84127e7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human love dogs', 'dogs are intelligent', 'dog eats meat', 'human love meat']\n"
          ]
        }
      ],
      "source": [
        "documents= ['Human love dogs.','Dogs are intelligent.', 'Dog eats meat','Human love meat']\n",
        "processed_docs=[doc.lower().replace('.','') for doc in documents]\n",
        "print(processed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab= {}\n",
        "count=0\n",
        "\n",
        "for doc in processed_docs:\n",
        "  for word in doc.split():\n",
        "    if word not in vocab:\n",
        "      count= count+1\n",
        "      vocab[word]= count\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDAiNzfAskby",
        "outputId": "dfabab0d-3cc4-46f5-aef2-d15e360e912a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'human': 1, 'love': 2, 'dogs': 3, 'are': 4, 'intelligent': 5, 'dog': 6, 'eats': 7, 'meat': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hot_encode(input_string):\n",
        "  onehot_encode=[]\n",
        "  for word in input_string.split():\n",
        "    temp=[0]*len(vocab)\n",
        "    if word in vocab:\n",
        "      temp[vocab[word]-1]=1\n",
        "    onehot_encode.append(temp)\n",
        "\n",
        "  return onehot_encode\n"
      ],
      "metadata": {
        "id": "TK_jhxVTtAcA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_docs[1])\n",
        "hot_encode(processed_docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMURWh-Dt79w",
        "outputId": "3aeb53b6-e8ad-42ce-b33e-37bb6754a99a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dogs are intelligent\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "data = [doc.lower().replace('.','').split() for doc in documents]\n",
        "print(data)\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "encoded_text= one_hot_encoder.fit_transform(data).toarray()\n",
        "\n",
        "\n",
        "print('Onehot encoded matrix', encoded_text )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wueuZGlCt-hD",
        "outputId": "20d29310-ef1d-422f-93e7-9ace7000b094"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['human', 'love', 'dogs'], ['dogs', 'are', 'intelligent'], ['dog', 'eats', 'meat'], ['human', 'love', 'meat']]\n",
            "Onehot encoded matrix [[0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 1. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of Words"
      ],
      "metadata": {
        "id": "YSwHY6WUyWUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "bow = count_vect.fit_transform(processed_docs)"
      ],
      "metadata": {
        "id": "FjUlidRLyYsA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT4QE9Zby0C7",
        "outputId": "4fbd66f5-f3d8-4e05-e356-2db5d243bd39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'human': 4,\n",
              " 'love': 6,\n",
              " 'dogs': 2,\n",
              " 'are': 0,\n",
              " 'intelligent': 5,\n",
              " 'dog': 1,\n",
              " 'eats': 3,\n",
              " 'meat': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = count_vect.transform([\"dogs are loyal to human\"])\n",
        "temp.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJn3M3_Ky2TO",
        "outputId": "38901168-a62f-42bd-a5c1-08552b237e49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BoW with binary vectors\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "count_vect.fit(processed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jo1SbPTzK3C",
        "outputId": "f73e52b0-bb50-4e0e-c313-d085151b95dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEhKe_tE3Zey"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of N-Grams"
      ],
      "metadata": {
        "id": "B4jyI3CSEy53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer(ngram_range=(1,3))\n",
        "bow = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "print(' vocab = ', count_vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOXWn3J9E2Cd",
        "outputId": "4d685035-f5fe-4df7-9f8a-67d47a6b5545"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " vocab =  {'human': 10, 'love': 15, 'dogs': 5, 'human love': 11, 'love dogs': 16, 'human love dogs': 12, 'are': 0, 'intelligent': 14, 'dogs are': 6, 'are intelligent': 1, 'dogs are intelligent': 7, 'dog': 2, 'eats': 8, 'meat': 18, 'dog eats': 3, 'eats meat': 9, 'dog eats meat': 4, 'love meat': 17, 'human love meat': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = count_vect.transform([\"dogs are loyal to human\"])\n",
        "temp.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZaMoo0FFHMe",
        "outputId": "53008033-30fd-4c8f-a34c-513f46ead491"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!!"
      ],
      "metadata": {
        "id": "VYcmlI0lFU7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF"
      ],
      "metadata": {
        "id": "LZDFW9nAFrSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf= TfidfVectorizer()\n",
        "\n",
        "bow_tfid = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "print('Words in vocabulary ', tfidf.get_feature_names_out())\n",
        "print('IDF for all words in vocab ', tfidf.idf_)\n",
        "\n",
        "\n",
        "print('TFIDF representation for all documents in our corpus',bow_tfid.toarray())\n",
        "\n",
        "temp = tfidf.transform([\"dogs are loyal to human\"])\n",
        "print('temp representation ' ,temp.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxA3iKmFO-R",
        "outputId": "7b66d43c-90d5-45cd-a708-6651a3102dc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in vocabulary  ['are' 'dog' 'dogs' 'eats' 'human' 'intelligent' 'love' 'meat']\n",
            "IDF for all words in vocab  [1.91629073 1.91629073 1.51082562 1.91629073 1.51082562 1.91629073\n",
            " 1.51082562 1.51082562]\n",
            "TFIDF representation for all documents in our corpus [[0.         0.         0.57735027 0.         0.57735027 0.\n",
            "  0.57735027 0.        ]\n",
            " [0.61761437 0.         0.48693426 0.         0.         0.61761437\n",
            "  0.         0.        ]\n",
            " [0.         0.61761437 0.         0.61761437 0.         0.\n",
            "  0.         0.48693426]\n",
            " [0.         0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.57735027]]\n",
            "temp representation  [[0.66767854 0.         0.52640543 0.         0.52640543 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "gpKGQOIkPQlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
        "\n",
        "sentiment labelled sentences dataset from UCI repository http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences is used.\n",
        "\n",
        "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. \n",
        "\n",
        "For a pre-trained embedding model, Google News vectors. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM is used"
      ],
      "metadata": {
        "id": "PB3UArXWPU_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "\n",
        "#basic imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "from time import time\n",
        "\n",
        "#pre-processing imports\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "8UnIZ7vyGlj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2a2c75-c799-4d1d-a87c-763f6a19c3c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=942c56ade8e1f7cf79f78e04389824185d857504bd27c9c1347a53ad1a698674\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pre-trained word2vec model and the dataset\n",
        "    \n",
        "from google.colab import files\n",
        "data_path= \"DATAPATH\"\n",
        "!wget -P DATAPATH https://text-segmentation-wordembedding.s3.amazonaws.com/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip DATAPATH/GoogleNews-vectors-negative300.bin.gz      \n",
        "path_to_model = 'DATAPATH/GoogleNews-vectors-negative300.bin'\n",
        "training_data_path = \"sentiment_sentences.txt\"\n",
        "      \n",
        "#Load W2V model. This will take some time. \n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print('done loading Word2Vec')\n",
        "\n",
        "#Read text data, cats.\n",
        "#the file path consists of tab separated sentences and cats.\n",
        "texts = []\n",
        "cats = []\n",
        "fh = open(training_data_path)\n",
        "for line in fh:\n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcSQJ-b4Rxtk",
        "outputId": "0bbaa133-5bf0-4d4a-c1f9-22884281e510"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-10 14:23:37--  https://text-segmentation-wordembedding.s3.amazonaws.com/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving text-segmentation-wordembedding.s3.amazonaws.com (text-segmentation-wordembedding.s3.amazonaws.com)... 54.231.172.81\n",
            "Connecting to text-segmentation-wordembedding.s3.amazonaws.com (text-segmentation-wordembedding.s3.amazonaws.com)|54.231.172.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘DATAPATH/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  41.9MB/s    in 36s     \n",
            "\n",
            "2022-10-10 14:24:13 (43.8 MB/s) - ‘DATAPATH/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n",
            "CPU times: user 39.3 s, sys: 5.18 s, total: 44.5 s\n",
            "Wall time: 44.8 s\n",
            "done loading Word2Vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspect the model\n",
        "word2vec_vocab = w2v_model.vocab.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZe9Oi4pR5iR",
        "outputId": "9722ca6b-7a94-4978-8573-55c22434db09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspect the dataset\n",
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHPtutAeR8tU",
        "outputId": "31ff9bd9-1a38-479b-d643-585d9453dee0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "Good case, Excellent value.\n",
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the text.\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-elQnKUSCFM",
        "outputId": "3b3422fc-00a1-4bae-fe36-35a3fc561153"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "['good', 'case', 'excellent', 'value']\n",
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a feature vector by averaging all embeddings for all sentences\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    i=0\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        if(count_for_this!=0):\n",
        "            feats.append(feat_for_this/count_for_this) \n",
        "        else:\n",
        "            feats.append(zero_vector)\n",
        "        i+=1\n",
        "    return feats\n",
        "\n",
        "\n",
        "train_vectors = embedding_feats(texts_processed)\n",
        "print(len(train_vectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XTV4gzASF6_",
        "outputId": "f195d63a-974d-4d8b-ae9a-e6a01ad2d88b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training logisticRegression \n",
        "classifier = LogisticRegression(random_state=1234)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E74yKOHoSJjT",
        "outputId": "44474f32-77a6-4880-b78a-91c5becf627e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8106666666666666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0\n",
            "       0.82      0.80      0.81       374\n",
            "          1\n",
            "       0.81      0.82      0.81       376\n",
            "\n",
            "    accuracy                           0.81       750\n",
            "   macro avg       0.81      0.81      0.81       750\n",
            "weighted avg       0.81      0.81      0.81       750\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7PFQXvxUryD"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}